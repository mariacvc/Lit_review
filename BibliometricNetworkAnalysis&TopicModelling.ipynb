{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bacf3e-b771-4f35-98f1-32f8bf8802ed",
   "metadata": {},
   "source": [
    "# Bibliometric network analysis & topic modelling\n",
    "\n",
    "Bibliometric data from academic databases can be used to find relationships between metadata (authors, titles, citations etc.) and discover dominant topics. In this kernel, we'll use the Metaknowledge package and an information science and bibliometrics dataset from Web of Science to perform network analysis and LDA topic modelling, along with visualizations. We'll try and answer the following questions:\n",
    "\n",
    "    Which of the top authors are also top co-authors?\n",
    "    What does the co-authorship network look like?\n",
    "    What are the dominant topics that emerge from these academic papers?\n",
    "\n",
    "https://www.kaggle.com/code/kruttika17/bibliometric-network-analysis-topic-modelling\n",
    "\n",
    "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b982c2c0-8f33-4a88-8f7b-0f29cbf6cb82",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d9c26-16c7-44ff-9a22-2c4060993d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import metaknowledge as mk\n",
    "import networkx as nx\n",
    "import community\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import CoherenceModel \n",
    "import re\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\n",
    "from community import community_louvain\n",
    "\n",
    "# Import the wordcloud library\n",
    "# from wordcloud import WordCloud# Join the different processed titles together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375eb04-2d8a-4270-9e1b-064b9195c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the information science and bibliometrics dataset\n",
    "RC = mk.RecordCollection('C:/Users/cata1/OneDrive - University of California, Davis/GEO200E_ResearchDesign/LitReview', cached = True)\n",
    "\n",
    "len(RC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b47c4a-86ff-4d7f-b47c-3317312e64f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing basic statistics about the data\n",
    "print(RC.glimpse())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cde318-f36a-4734-8146-990d476ee319",
   "metadata": {},
   "source": [
    "# Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad71fc3-1475-4f24-aa0f-c59303966df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the co-author network \n",
    "coauth_net = RC.networkCoAuthor()\n",
    "coauth_net\n",
    "\n",
    "# Printing the network stats\n",
    "print(mk.graphStats(coauth_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d33816-353d-4268-ab55-f5223c311107",
   "metadata": {},
   "source": [
    "There are 857 nodes (authors) in the network who are connected by 2752 edges. Of these authors, 17 are isolates (unconnected to others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2afb51e-4de7-4d8c-b873-5bfd37206dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk.dropEdges(coauth_net, minWeight = 2, dropSelfLoops = True)\n",
    "\n",
    "components = list(nx.connected_components(coauth_net))\n",
    "giant_coauth = coauth_net.subgraph(max(components, key=len))\n",
    "\n",
    "print(mk.graphStats(giant_coauth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f532ba-465d-4aea-b2fc-29573b50e0fd",
   "metadata": {},
   "source": [
    "we are left with 7 authors, all of whom have at least two co-authors. We can see the graph density has gone up because of our filtering criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d6a83-80c4-4e77-8c2e-aca92aafc02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing centrality scores\n",
    "deg = nx.degree_centrality(giant_coauth)\n",
    "clo = nx.closeness_centrality(giant_coauth)\n",
    "bet = nx.betweenness_centrality(giant_coauth)\n",
    "eig = nx.eigenvector_centrality(giant_coauth)\n",
    "\n",
    "# Saving the scores as a dataframe\n",
    "cent_df = pd.DataFrame.from_dict([deg, clo, bet, eig])\n",
    "cent_df = pd.DataFrame.transpose(cent_df)\n",
    "cent_df.columns = [\"degree\", \"closeness\", \"betweenness\", \"eigenvector\"]\n",
    "\n",
    "# Printing the top 10 co-authors by degree centrality score\n",
    "cent_df.sort_values(\"degree\", ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5acaf-a772-4476-aafc-7f84b4d43822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top 10 co-authors by degree centrality score\n",
    "sns.set(font_scale=.75)\n",
    "cent_df_d10 = cent_df.sort_values('degree', ascending = False)[:10]\n",
    "cent_df_d10.index.name = \"author\"\n",
    "cent_df_d10.reset_index(inplace=True)\n",
    "print()\n",
    "plt.figure(figsize=(10,7))\n",
    "ax = sns.barplot(y = \"author\", x = \"degree\", data = cent_df_d10, palette = \"Set2\");\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title(\"Top 10 authors in co-author graph\", fontsize = 18)\n",
    "ax.set_ylabel(\"Authors\", fontsize=14);\n",
    "ax.set_xlabel(\"Degree centrality\", fontsize=14);\n",
    "ax.tick_params(axis = 'both', which = 'major', labelsize = 14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2100e-143e-4d83-9692-96a961a969ca",
   "metadata": {},
   "source": [
    "The top 3 authors in the co-author network are the same as the top 3 authors in the original Record Collection. However, there are 5 authors in the original top 10 who are missing from the top 10 co-authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f956b32-8dac-4354-8416-e2de6e5b10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network visualizations can be difficult and confusing. There are several possible layouts, but we'll use the \"spring layout\" which results in a more aesthetic graph.\n",
    "# Visualizing the co-author network\n",
    "plt.figure(figsize = (10, 7))\n",
    "size = [2000 * eig[node] for node in giant_coauth]\n",
    "nx.draw_spring(giant_coauth, node_size = size, with_labels = True, font_size = 5,\n",
    "               node_color = \"#FFFFFF\", edge_color = \"#D4D5CE\", alpha = .95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41601d27-05cb-4350-995b-f672b486e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all of them\n",
    "eig2 = nx.eigenvector_centrality(coauth_net)\n",
    "# Visualizing the co-author network\n",
    "plt.figure(figsize = (10, 7))\n",
    "size = [2000 * eig2[node] for node in coauth_net]\n",
    "nx.draw_spring(coauth_net, node_size = size, with_labels = True, font_size = 6,\n",
    "               node_color = \"#FFFFFF\", edge_color = \"#D4D5CE\", alpha = .95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d0a76-71fa-4d70-87a7-e54b79cda97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community detection\n",
    "partition = community_louvain.best_partition(giant_coauth) \n",
    "modularity = community_louvain.modularity(partition, giant_coauth)\n",
    "print(\"Modularity:\", modularity)\n",
    "\n",
    "# Visualizing the communities\n",
    "# Generates a different graph each time\n",
    "plt.figure(figsize = (10, 7))\n",
    "colors = [partition[n] for n in giant_coauth.nodes()]\n",
    "my_colors = plt.cm.Set2 \n",
    "nx.draw(giant_coauth, node_color=colors, cmap = my_colors, edge_color = \"#D4D5CE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e1f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community detection \n",
    "partition = community_louvain.best_partition(coauth_net) \n",
    "modularity = community_louvain.modularity(partition, coauth_net)\n",
    "print(\"Modularity:\", modularity)\n",
    "\n",
    "# Visualizing the communities\n",
    "# Generates a different graph each time\n",
    "plt.figure(figsize = (10, 7))\n",
    "colors = [partition[n] for n in coauth_net.nodes()]\n",
    "my_colors = plt.cm.Set2 \n",
    "nx.draw(coauth_net, node_color=colors, cmap = my_colors, edge_color = \"#D4D5CE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b5e6f-14f4-44c3-a0ae-fd3afc15edb2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Topic Modelling\n",
    "\n",
    "The Metaknowledge function forNLP() creates a Pandas-friendly dictionary where each row is a record from the RecordCollection, and the columns contain textual data (id, title, publication year, keywords and the abstract). Its results are not reproducible - the records appear to be shuffled each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84401dd1-190b-4e3f-89cf-390695346375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform the record collection into a format for use with natural language processing applications\n",
    "data = RC.forNLP(\"topic_model.csv\", lower=True, removeNumbers=True, removeNonWords=True, removeWhitespace=True)\n",
    "\n",
    "# Convert the raw text into a list.\n",
    "docs = data['abstract']\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b601ff4-0529-40ec-8d06-5e13bbd21ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining a function to clean the text\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "def clean(docs):\n",
    "    # Insert function for preprocessing the text\n",
    "    def sent_to_words(sentences):\n",
    "        for sentence in sentences:\n",
    "            yield (simple_preprocess(str(sentence), deacc = True))\n",
    "    # Tokenize the text\n",
    "    tokens = sent_to_words(docs)\n",
    "    # Create stopwords set\n",
    "    #stop = set(stopwords.words(\"english\"))\n",
    "    # Create lemmatizer\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    # Remove stopwords from text\n",
    "    #tokens_stopped = [[word for word in post if word not in stop] for post in tokens]\n",
    "    # Lemmatize text\n",
    "    tokens_cleaned = [[lmtzr.lemmatize(word) for word in post] for post in tokens]\n",
    "    # Return cleaned text\n",
    "    return tokens_cleaned\n",
    "\n",
    "# Cleaning up the raw documents\n",
    "cleaned_docs = clean(docs)\n",
    "cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ffbbc-4fc9-4cc4-bd11-f4d6469064cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary\n",
    "id2word = corpora.Dictionary(cleaned_docs)\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010794b-d060-4019-b3b7-07e5840401c2",
   "metadata": {},
   "source": [
    "There are 4731 unique words in the text. We'll filter out infrequent and overly frequent words from the dictionary, as this can improve the topic model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91326c1b-3ef6-4223-95a0-8c4ea89b9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering infrequent and over frequent words\n",
    "id2word.filter_extremes(no_below=5, no_above=0.5)\n",
    "# Creating a document-term matrix\n",
    "corpus = [id2word.doc2bow(doc) for doc in cleaned_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2ba06-e983-4401-82ff-3f7499644d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an LDA model with 5 topics\n",
    "model = ldamodel.LdaModel(corpus = corpus, num_topics = 5, id2word = id2word, \n",
    "                              passes = 10, update_every = 1, chunksize = 1000, per_word_topics = True, random_state = 1)\n",
    "# Printing the topic-word distributions\n",
    "pprint(model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2b517-9538-44ee-bde8-75fb6707325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(model, corpus, id2word, mds = \"tsne\")\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c9fe9",
   "metadata": {},
   "source": [
    "# pyBibX-00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requiered libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from pyBibX.base import pbx_probe\n",
    "\n",
    "#from google.colab import data_table\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c91ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "# Read data into papers\n",
    "file_name = 'C:/Users/cata1/OneDrive - University of California, Davis/GEO200E_ResearchDesign/LitReview/savedrecs.bib'\n",
    "database = 'savedrecs'\n",
    "bibfile = pbx_probe(file_bib = file_name, db = database)\n",
    "#papers = pd.read_csv('C:/Users/cata1/OneDrive - University of California, Davis/GEO200E_ResearchDesign/LitReview/savedrecs.csv')# Print head\n",
    "#papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db47ef-f907-4026-b3ec-9a1a28a81de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate EDA (Exploratory Data Analysis) Report\n",
    "report = bibfile.eda_bib()\n",
    "\n",
    "# Check report\n",
    "report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Docs IDs\n",
    "data_ID = pd.DataFrame(bibfile.table_id_doc)\n",
    "display(data_ID.iloc[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Docs IDs per Type\n",
    "data_Type = pd.DataFrame(bibfile.id_doc_types())\n",
    "display(data_Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Authors IDs\n",
    "data_Authors = pd.DataFrame(bibfile.table_id_aut)\n",
    "display(data_Authors.iloc[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83307910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Sources IDs\n",
    "data_Sources = pd.DataFrame(bibfile.table_id_jou)\n",
    "display(data_Sources.iloc[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb394b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Institutions IDs\n",
    "data_Uni = pd.DataFrame(bibfile.table_id_uni)\n",
    "display(data_Uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe178493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Countries IDs\n",
    "data_Countries = pd.DataFrame(bibfile.table_id_ctr)\n",
    "display(data_Countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af8750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Keywords IDs\n",
    "data_Key = pd.DataFrame(bibfile.table_id_kwa)\n",
    "display(data_Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud\n",
    "bibfile.word_cloud_plot(entry = 'abs', size_x= 15, size_y= 10, wordsn=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b034389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Table\n",
    "table = PrettyTable()\n",
    "data_wd = bibfile.ask_gpt_wd\n",
    "table.field_names = ['Wprd', 'Importance']\n",
    "for key, value in data_wd.items():\n",
    "    table.add_row([key, round(value,4)])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ddba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Grams\n",
    "bibfile.get_top_ngrams(view = 'notebook', entry = 'kwp', ngrams = 3, stop_words = [], rmv_custom_words = [], wordsn = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542c61c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
